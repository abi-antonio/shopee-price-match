{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Introduction\nThis notebook will identify similar products based solely on their product names.\n\nTo do this the following steps are taken\n1. Convert product title to a vector\n2. Compute a similarity score and identify similar products\n3. Identify the optimal threshold\n4. Create submission file\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport csv\nfrom sklearn.feature_extraction.text import TfidfVectorizer","metadata":{"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# Load data\ndf = pd.read_csv(\"/kaggle/input/shopee-product-matching/train.csv\")\ndf.head()","metadata":{"trusted":true},"execution_count":2,"outputs":[{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \n0                          Paper Bag Victoria Secret    249114794  \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045  \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891  \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188  \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Step 0: Build the actual list of matches\n\nFirst things first, let's determine what should be the correct output of our submission file. The label_group column contains which group a posting belongs, so postings in the same label_group are deemed as similar.\n\nTo do this, we'll group the rows based on their `label_group` and create a list of `posting_id`s that belong to each label_group. Afterwards, we'll add a new column, `actual` to the dataframe containing the posting_ids that belong to the row's label_group","metadata":{}},{"cell_type":"code","source":"# get list of posting_ids per label group\nactual = df.groupby('label_group').posting_id.agg('unique').to_dict()\n\n# add new column, actual, into the dataframe based on row's label_group\ndf['actual'] = df.label_group.map(actual)\n\ndf.head()","metadata":{"trusted":true},"execution_count":3,"outputs":[{"execution_count":3,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                                 actual  \n0   [train_129225211, train_2278313361]  \n1  [train_3386243561, train_3423213080]  \n2  [train_2288590299, train_3803689425]  \n3  [train_2406599165, train_3342059966]  \n4   [train_3369186413, train_921438619]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>actual</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>[train_129225211, train_2278313361]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>[train_3386243561, train_3423213080]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>[train_2288590299, train_3803689425]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>[train_2406599165, train_3342059966]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>[train_3369186413, train_921438619]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"While making this notebook, I noticed something interesting with row 3. It should only match postings *train_2406599165* and *train_3342059966* but later on in the notebook we'll see that *train_1744956981* has the exact same title but the label_group is different.\n\nIf we look at their image_phash, we can see that they are diifferent meaning they were posted with different images. This might be the reason why they are classified as different groups. It will be good to take note of this when combining the predictions made in this notebook with the predictions of a model that analyzes the images but for now, we'll just ignore it.","metadata":{}},{"cell_type":"code","source":"# See an example where titles are exactly the same but label group is different\ndf.loc[df['posting_id'].isin(['train_2406599165', 'train_1744956981'])]","metadata":{"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"             posting_id                                 image  \\\n3      train_2406599165  00117e4fc239b1b641ff08340b429633.jpg   \n28878  train_1744956981  d81acc9a273166cbfa8c85ca34d31f5f.jpg   \n\n            image_phash                                              title  \\\n3      8514fc58eafea283  Daster Batik Lengan pendek - Motif Acak / Camp...   \n28878  abc28c0dd1a4d37e  Daster Batik Lengan pendek - Motif Acak / Camp...   \n\n       label_group                                actual  \n3       4093212188  [train_2406599165, train_3342059966]  \n28878   3150867956  [train_1508100548, train_1744956981]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>actual</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>[train_2406599165, train_3342059966]</td>\n    </tr>\n    <tr>\n      <th>28878</th>\n      <td>train_1744956981</td>\n      <td>d81acc9a273166cbfa8c85ca34d31f5f.jpg</td>\n      <td>abc28c0dd1a4d37e</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>3150867956</td>\n      <td>[train_1508100548, train_1744956981]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's export this to a csv file to use later when we compare with our submission file","metadata":{}},{"cell_type":"code","source":"# convert the actual column into a space delimited string, similar to the output file\n#df['actual_str'] = df.apply(lambda row: ' '.join(row['actual']), axis=1)\n\n# export the posting_id and actual_str columns\n#df[['posting_id', 'actual_str']].to_csv('actuals.csv', index=False)\n\n# examine the created file\n#actuals_df = pd.read_csv('actuals.csv')\n#actuals_df.head()","metadata":{"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# Step 1: Convert product title to vector\nLet's begin! Computers are good with processing numbers but they don't know how to process text. To resolve this, we want to find a way to represent the titles as numbers instead of text. We'll use the **TF-IDF algorithm** to represent the text as a vector. The vector contains a weight for each word in the vocabulary. This will show which words best represent the title.\n\nFirst, we extract the product titles. Then we use sklearn's TfidfVectorizer to generate the weights per word","metadata":{}},{"cell_type":"code","source":"# get titles of products\ntitles = df['title']\n\n# create a tfidfvectorizer model\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_weights = tfidf_vectorizer.fit_transform(titles)\n\ntitles.head()","metadata":{"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"0                            Paper Bag Victoria Secret\n1    Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...\n2          Maling TTS Canned Pork Luncheon Meat 397 gr\n3    Daster Batik Lengan pendek - Motif Acak / Camp...\n4                    Nescafe \\xc3\\x89clair Latte 220ml\nName: title, dtype: object"},"metadata":{}}]},{"cell_type":"markdown","source":"Let's examine the weights generated","metadata":{}},{"cell_type":"code","source":"print(\"tfidf_weights.shape=\", tfidf_weights.shape)\n\n# tfidf_weights is a sparse matrix. Let's convert it to a data frame to take a peak at the data\n#df_tfidf_weights = pd.DataFrame(data = tfidf_weights.toarray(), columns=tfidf_vectorizer.get_feature_names())\n#df_tfidf_weights.head()","metadata":{"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"tfidf_weights.shape= (34250, 25069)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"We can see that from the 34,250 products there are 25,069 unique words used in the titles","metadata":{}},{"cell_type":"markdown","source":"# Step 2: Compute similarity score and identify similar products\n\nNext we need to compute how similar the products are to each other. Since there are 34,250 products, we'll be generating a 34,250 x 34,250 matrix.\n\nThe score can be computed by using either Euclidean distance or cosine similarity. *(This [article](https://www.baeldung.com/cs/euclidean-distance-vs-cosine-similarity) explains the difference really well)*\n- Euclidean distance, aka L2-norm, measures the distance between 2 points in a vector space\n- Cosine similarity, on the otherhand, does not treat the data as points rather as vectors from the origin the measure. This way alows us to compute the anfular distance, which basically means how close the vectors are to each other.\n\nWe'll be computing both scores then see later which one performs better. Also for now, let's only say that products are the same only if their titles are exactly the same\n\nNote: To avoid any issues regarding space allocation for the 70,000 records in the hidden test set, we'll be copmputing the distances in chunks","metadata":{}},{"cell_type":"code","source":"# set the chuck size\nCHUNK_SIZE = 1024*4\nprint(\"CHUNK_SIZE=\", CHUNK_SIZE)\n\n# compute how many chunks we need to process\nCHUNKS = df.shape[0]//CHUNK_SIZE if df.shape[0] % CHUNK_SIZE == 0 else df.shape[0]//CHUNK_SIZE + 1\nprint(\"CHUNKS=\", CHUNKS)","metadata":{"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"CHUNK_SIZE= 4096\nCHUNKS= 9\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Euclidean Distance","metadata":{}},{"cell_type":"markdown","source":"Below is the code to compute for the euclidean distance between 2 product titles","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics.pairwise import euclidean_distances\n\ndef predict_by_euclidean(threshold, verbose=True):\n    print(\"############ Predict using euclidean distance\")\n    print(\"############ THRESHOLD = \", threshold)\n    \n    # initialize the list where we'll store the predictions\n    predictions = []\n    maxs = []\n\n    # compute the euclidean distance per chunk\n    for i in range(CHUNKS):\n        # compute the start and end indexes of the chunk\n        start_index = i * CHUNK_SIZE\n        end_index = start_index + CHUNK_SIZE\n        if(end_index > df.shape[0]):\n            end_index = df.shape[0]\n        \n        if verbose:\n            print(\"Processing records\", start_index, \"to\", end_index)\n\n        e_distance = euclidean_distances(tfidf_weights[start_index:end_index], tfidf_weights)\n\n        # loop through each record\n        for j in range(0,e_distance.shape[0]):\n            # get the indices of products with exactly the same title\n            indices = np.where(e_distance[j,:] <= threshold)\n\n            # get posting ids\n            posting_ids = df.loc[indices,'posting_id'].to_list()\n\n            # add these indices to a list\n            predictions.append(posting_ids)\n        \n        maxs.append(e_distance.max())\n    \n    if verbose:\n        print(\"Largest difference=\", e_distance.max())\n\n    return predictions","metadata":{"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"markdown","source":"The code below will call the function above and add our predictions to the dataframe as a new column, e_prediction.","metadata":{}},{"cell_type":"code","source":"df['e_prediction'] = predict_by_euclidean(0.9)\ndf.head()","metadata":{"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"Processing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\nLargest difference= 1.4142135623730956\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                                 actual  \\\n0   [train_129225211, train_2278313361]   \n1  [train_3386243561, train_3423213080]   \n2  [train_2288590299, train_3803689425]   \n3  [train_2406599165, train_3342059966]   \n4   [train_3369186413, train_921438619]   \n\n                                        e_prediction  \n0                [train_129225211, train_2278313361]  \n1  [train_3386243561, train_860027362, train_3423...  \n2               [train_2288590299, train_3803689425]  \n3  [train_2406599165, train_3576714541, train_150...  \n4                                 [train_3369186413]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>actual</th>\n      <th>e_prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>[train_129225211, train_2278313361]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>[train_3386243561, train_3423213080]</td>\n      <td>[train_3386243561, train_860027362, train_3423...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>[train_2288590299, train_3803689425]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>[train_2406599165, train_3342059966]</td>\n      <td>[train_2406599165, train_3576714541, train_150...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>[train_3369186413, train_921438619]</td>\n      <td>[train_3369186413]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"## Cosine Similarity\n\nTo compute the cosine similarity we just simple need to take the dot product between 2 postings","metadata":{}},{"cell_type":"code","source":"import scipy.sparse as sp\n\ndef predict_by_cosine_similarity(threshold, verbose=True):\n    print(\"############ Predict using cosine similarity\")\n    print(\"############ THRESHOLD = \", threshold)\n    \n    # initialize the list where we'll store the predictions\n    predictions = []\n\n    # compute the cosine similarity distance per chunk\n    for i in range(CHUNKS):\n        # compute the start and end indexes of the chunk\n        start_index = i * CHUNK_SIZE\n        end_index = start_index + CHUNK_SIZE\n        if(end_index > df.shape[0]):\n            end_index = df.shape[0]\n        \n        if verbose:\n            print(\"Processing records\", start_index, \"to\", end_index)\n\n        # compute cosine similarity distance\n        cs_distance = tfidf_weights[start_index:end_index].dot(tfidf_weights.T)\n        #print(\"cs_distance.shape = \",cs_distance.shape)\n\n        # get posting_ids where cosine similarity distance >= threshold\n        for j in range(0,cs_distance.shape[0]):\n            # get values that are <= threshold\n            valid_distances = cs_distance[j,:] >= threshold\n            #print(\"valid_distances.shape = \",valid_distances.shape)\n            \n            # get indices from cs_distance matrix\n            row_idx, col_idx, _ = sp.find(valid_distances)\n\n            # get corresponding posting_id of indices\n            posting_ids = df.loc[col_idx,'posting_id'].to_list()\n\n            # add posting_ids to predictions list\n            predictions.append(posting_ids)\n    \n    return predictions","metadata":{"trusted":true},"execution_count":27,"outputs":[]},{"cell_type":"code","source":"df['cs_prediction'] = predict_by_cosine_similarity(0.7)\ndf.head()","metadata":{"trusted":true},"execution_count":15,"outputs":[{"name":"stdout","text":"Processing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\n","output_type":"stream"},{"execution_count":15,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                                 actual  \\\n0   [train_129225211, train_2278313361]   \n1  [train_3386243561, train_3423213080]   \n2  [train_2288590299, train_3803689425]   \n3  [train_2406599165, train_3342059966]   \n4   [train_3369186413, train_921438619]   \n\n                                        e_prediction  \\\n0                [train_129225211, train_2278313361]   \n1  [train_3386243561, train_860027362, train_3423...   \n2               [train_2288590299, train_3803689425]   \n3  [train_2406599165, train_3576714541, train_150...   \n4                                 [train_3369186413]   \n\n                                       cs_prediction  \n0                [train_129225211, train_2278313361]  \n1                                 [train_3386243561]  \n2                                 [train_2288590299]  \n3  [train_2406599165, train_3576714541, train_150...  \n4                                 [train_3369186413]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>actual</th>\n      <th>e_prediction</th>\n      <th>cs_prediction</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>[train_129225211, train_2278313361]</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>[train_3386243561, train_3423213080]</td>\n      <td>[train_3386243561, train_860027362, train_3423...</td>\n      <td>[train_3386243561]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>[train_2288590299]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>[train_2406599165, train_3342059966]</td>\n      <td>[train_2406599165, train_3576714541, train_150...</td>\n      <td>[train_2406599165, train_3576714541, train_150...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>[train_3369186413, train_921438619]</td>\n      <td>[train_3369186413]</td>\n      <td>[train_3369186413]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Step 3: Identifying the optimal threshold\n\nThe `predict_by_euclidean` and `predict_by_cosine_similarity` functions that we created above determine that products are similar based on a threshold. There are different possible values that we can use as a threshold so let's identify which one gives us the best accuracy\n- **Euclidean Distance** - Based on our data, the smallest distance we can compute is 0 and largest is around 1.41. Therefore, we should look for thresholds between 0 to 1.41. Note that titles are exactly the same if the euclidean distance is 0\n- **Cosine Similarity** - The values we can compute from cosine similarity ranges from 0 to 1. Also note that titles are exactly the same if we get a value of 1\n\nFor this competition, accuracy is determined based on the F1 score and can be computed as follows: (Formula was referenced from this [notebook](https://www.kaggle.com/c/shopee-product-matching/discussion/225093))\n\n$$\nF1 Metric = \\frac{2 * \\mid X \\cap Y \\mid}{\\mid X\\mid + \\mid Y\\mid}\n$$","metadata":{}},{"cell_type":"code","source":"def getMetric(col):\n    def f1score(row):\n        n = len( np.intersect1d(row.actual,row[col]) )\n        return 2*n / (len(row.actual)+len(row[col]))\n    return f1score","metadata":{"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"markdown","source":"Let's see what's the optimal threshold if we use euclidean distance","metadata":{}},{"cell_type":"code","source":"# set threshold size\nTHRESHOLD_VALUES = [0, 0.3, 0.6, 0.9, 1, 1.3] # Note: The largest distance is around 1.41 so going beyond 1.3 is no longer productive\ncv_scores = []\n\nfor threshold in THRESHOLD_VALUES:\n    # get predictions using threshold\n    predictions = predict_by_euclidean(threshold)\n    \n    # update data frame with predictions\n    df['e_prediction'] = predictions\n    \n    # compute f1 score per row\n    df['f1_e'] = df.apply(getMetric('e_prediction'),axis=1)\n    \n    # get cv score\n    cv_score = df.f1_e.mean()\n    cv_scores.append(cv_score)\n    print('cv_score =', cv_score)\n\ne_method_accuracies = pd.DataFrame({'threshold':THRESHOLD_VALUES, 'accuracy':cv_scores})\ne_method_accuracies.sort_values('accuracy', ascending=False)\n","metadata":{"scrolled":true,"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"############ THRESHOLD =  0\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\nLargest difference= 1.4142135623730956\ncv_score = 0.49025587795310027\n############ THRESHOLD =  0.3\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\nLargest difference= 1.4142135623730956\ncv_score = 0.49845215989690755\n############ THRESHOLD =  0.6\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\nLargest difference= 1.4142135623730956\ncv_score = 0.5609835105215714\n############ THRESHOLD =  0.9\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\nLargest difference= 1.4142135623730956\ncv_score = 0.6459987805022945\n############ THRESHOLD =  1\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\nLargest difference= 1.4142135623730956\ncv_score = 0.6376343839173806\n############ THRESHOLD =  1.3\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\nLargest difference= 1.4142135623730956\ncv_score = 0.12435411211244936\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"   threshold  accuracy\n3        0.9  0.645999\n4        1.0  0.637634\n2        0.6  0.560984\n1        0.3  0.498452\n0        0.0  0.490256\n5        1.3  0.124354","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>threshold</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>3</th>\n      <td>0.9</td>\n      <td>0.645999</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.0</td>\n      <td>0.637634</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.6</td>\n      <td>0.560984</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.3</td>\n      <td>0.498452</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>0.0</td>\n      <td>0.490256</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>1.3</td>\n      <td>0.124354</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Now, let's see what's the optimal threshold for cosine similarity","metadata":{}},{"cell_type":"code","source":"THRESHOLD_VALUES = [1, 0.9, 0.95, 0.8, 0.85, 0.7, 0.75, 0.6, 0.65, 0.5, 0.55, 0.4]\ncv_scores = []\n\nfor threshold in THRESHOLD_VALUES:\n    print(\"############ THRESHOLD = \", threshold)\n    # get predictions using threshold\n    predictions = predict_by_cosine_similarity(threshold)\n    \n    # update data frame with predictions\n    df['cs_prediction'] = predictions\n    \n    # compute f1 score per row\n    df['f1_cs'] = df.apply(getMetric('cs_prediction'),axis=1)\n    \n    # get cv score\n    cv_score = df.f1_cs.mean()\n    cv_scores.append(cv_score)\n    print('cv_score =', cv_score)\n\ncs_method_accuracies = pd.DataFrame({'threshold':THRESHOLD_VALUES, 'accuracy':cv_scores})\ncs_method_accuracies.sort_values('accuracy', ascending=False)","metadata":{"scrolled":true,"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"############ THRESHOLD =  1\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.3201769568160777\n############ THRESHOLD =  0.9\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.5211588666283922\n############ THRESHOLD =  0.95\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.500532437967436\n############ THRESHOLD =  0.8\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.570925511453134\n############ THRESHOLD =  0.85\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.5459528655522399\n############ THRESHOLD =  0.7\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.617276102185251\n############ THRESHOLD =  0.75\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.5952413831590816\n############ THRESHOLD =  0.6\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.6452882123688801\n############ THRESHOLD =  0.65\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.6344577176578454\n############ THRESHOLD =  0.5\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.6376343839173806\n############ THRESHOLD =  0.55\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.6486870337299409\n############ THRESHOLD =  0.4\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\ncv_score = 0.566241908931998\n","output_type":"stream"},{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"    threshold  accuracy\n10       0.55  0.648687\n7        0.60  0.645288\n9        0.50  0.637634\n8        0.65  0.634458\n5        0.70  0.617276\n6        0.75  0.595241\n3        0.80  0.570926\n11       0.40  0.566242\n4        0.85  0.545953\n1        0.90  0.521159\n2        0.95  0.500532\n0        1.00  0.320177","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>threshold</th>\n      <th>accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>10</th>\n      <td>0.55</td>\n      <td>0.648687</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>0.60</td>\n      <td>0.645288</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>0.50</td>\n      <td>0.637634</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>0.65</td>\n      <td>0.634458</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>0.70</td>\n      <td>0.617276</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>0.75</td>\n      <td>0.595241</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.80</td>\n      <td>0.570926</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>0.40</td>\n      <td>0.566242</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.85</td>\n      <td>0.545953</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.90</td>\n      <td>0.521159</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.95</td>\n      <td>0.500532</td>\n    </tr>\n    <tr>\n      <th>0</th>\n      <td>1.00</td>\n      <td>0.320177</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"Comparing the euclidean and cosine similarity methods, it looks like cosine similarity has the best accuracy so for now let's try to create our submission file using this method and threshold.","metadata":{}},{"cell_type":"code","source":"print(\"Best accuracy from cosine method = \", cs_method_accuracies['accuracy'].max())\nprint(\"Best accuracy from euclidean method =\", e_method_accuracies['accuracy'].max())\n\nif cs_method_accuracies['accuracy'].max() > e_method_accuracies['accuracy'].max():\n    prediction_col = 'cs_prediction'\n    threshold = cs_method_accuracies.loc[cs_method_accuracies.accuracy == cs_method_accuracies['accuracy'].max(), 'threshold'].item()\nelse:\n    prediction_col = 'e_prediction'\n    threshold = e_method_accuracies.loc[e_method_accuracies.accuracy == e_method_accuracies['accuracy'].max(), 'threshold'].item()\n    \nprint(\"Use\", prediction_col, \" column with \", threshold, \" threshold for submission file\")\n    \n    ","metadata":{"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Best accuracy from cosine method =  0.6486870337299409\nBest accuracy from euclidean method = 0.6459987805022945\nUse cs_prediction  column with  0.55  threshold for submission file\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Step 4: Create the submission file\n\nTo create the submission file, let's generate the predictions using the ideal threshold","metadata":{}},{"cell_type":"code","source":"if prediction_col == 'e_prediction':\n    predictions = predict_by_euclidean(threshold)\nelse:\n    predictions = predict_by_cosine_similarity(threshold)\n    \n# update data frame with predictions\ndf['predictions_final'] = predictions","metadata":{"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"############ Predict using cosine similarity\n############ THRESHOLD =  0.55\nProcessing records 0 to 4096\nProcessing records 4096 to 8192\nProcessing records 8192 to 12288\nProcessing records 12288 to 16384\nProcessing records 16384 to 20480\nProcessing records 20480 to 24576\nProcessing records 24576 to 28672\nProcessing records 28672 to 32768\nProcessing records 32768 to 34250\n","output_type":"stream"}]},{"cell_type":"code","source":"df['matches'] = df.apply(lambda row: ' '.join(row['predictions_final']), axis=1)\ndf.head()","metadata":{"_kg_hide-input":false,"_kg_hide-output":false,"trusted":true},"execution_count":30,"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"         posting_id                                 image       image_phash  \\\n0   train_129225211  0000a68812bc7e98c42888dfb1c07da0.jpg  94974f937d4c2433   \n1  train_3386243561  00039780dfc94d01db8676fe789ecd05.jpg  af3f9460c2838f0f   \n2  train_2288590299  000a190fdd715a2a36faed16e2c65df7.jpg  b94cb00ed3e50f78   \n3  train_2406599165  00117e4fc239b1b641ff08340b429633.jpg  8514fc58eafea283   \n4  train_3369186413  00136d1cf4edede0203f32f05f660588.jpg  a6f319f924ad708c   \n\n                                               title  label_group  \\\n0                          Paper Bag Victoria Secret    249114794   \n1  Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...   2937985045   \n2        Maling TTS Canned Pork Luncheon Meat 397 gr   2395904891   \n3  Daster Batik Lengan pendek - Motif Acak / Camp...   4093212188   \n4                  Nescafe \\xc3\\x89clair Latte 220ml   3648931069   \n\n                                 actual  \\\n0   [train_129225211, train_2278313361]   \n1  [train_3386243561, train_3423213080]   \n2  [train_2288590299, train_3803689425]   \n3  [train_2406599165, train_3342059966]   \n4   [train_3369186413, train_921438619]   \n\n                                        e_prediction  \\\n0  [train_129225211, train_655833377, train_34979...   \n1  [train_3386243561, train_1816968361, train_366...   \n2  [train_2288590299, train_597673068, train_3135...   \n3  [train_2406599165, train_2088327894, train_975...   \n4  [train_3369186413, train_2223127215, train_105...   \n\n                                       cs_prediction      f1_e     f1_cs  \\\n0                [train_129225211, train_2278313361]  0.050633  1.000000   \n1  [train_3386243561, train_1816968361, train_212...  0.028369  0.200000   \n2               [train_2288590299, train_3803689425]  0.444444  1.000000   \n3  [train_2406599165, train_3576714541, train_270...  0.019231  0.181818   \n4                                 [train_3369186413]  0.100000  0.666667   \n\n                                   predictions_final  \\\n0                [train_129225211, train_2278313361]   \n1  [train_3386243561, train_860027362, train_7788...   \n2               [train_2288590299, train_3803689425]   \n3  [train_2406599165, train_3576714541, train_150...   \n4                                 [train_3369186413]   \n\n                                             matches  \n0                   train_129225211 train_2278313361  \n1  train_3386243561 train_860027362 train_7788161...  \n2                  train_2288590299 train_3803689425  \n3  train_2406599165 train_3576714541 train_150810...  \n4                                   train_3369186413  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>image</th>\n      <th>image_phash</th>\n      <th>title</th>\n      <th>label_group</th>\n      <th>actual</th>\n      <th>e_prediction</th>\n      <th>cs_prediction</th>\n      <th>f1_e</th>\n      <th>f1_cs</th>\n      <th>predictions_final</th>\n      <th>matches</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>0000a68812bc7e98c42888dfb1c07da0.jpg</td>\n      <td>94974f937d4c2433</td>\n      <td>Paper Bag Victoria Secret</td>\n      <td>249114794</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>[train_129225211, train_655833377, train_34979...</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>0.050633</td>\n      <td>1.000000</td>\n      <td>[train_129225211, train_2278313361]</td>\n      <td>train_129225211 train_2278313361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>00039780dfc94d01db8676fe789ecd05.jpg</td>\n      <td>af3f9460c2838f0f</td>\n      <td>Double Tape 3M VHB 12 mm x 4,5 m ORIGINAL / DO...</td>\n      <td>2937985045</td>\n      <td>[train_3386243561, train_3423213080]</td>\n      <td>[train_3386243561, train_1816968361, train_366...</td>\n      <td>[train_3386243561, train_1816968361, train_212...</td>\n      <td>0.028369</td>\n      <td>0.200000</td>\n      <td>[train_3386243561, train_860027362, train_7788...</td>\n      <td>train_3386243561 train_860027362 train_7788161...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>000a190fdd715a2a36faed16e2c65df7.jpg</td>\n      <td>b94cb00ed3e50f78</td>\n      <td>Maling TTS Canned Pork Luncheon Meat 397 gr</td>\n      <td>2395904891</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>[train_2288590299, train_597673068, train_3135...</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>0.444444</td>\n      <td>1.000000</td>\n      <td>[train_2288590299, train_3803689425]</td>\n      <td>train_2288590299 train_3803689425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>00117e4fc239b1b641ff08340b429633.jpg</td>\n      <td>8514fc58eafea283</td>\n      <td>Daster Batik Lengan pendek - Motif Acak / Camp...</td>\n      <td>4093212188</td>\n      <td>[train_2406599165, train_3342059966]</td>\n      <td>[train_2406599165, train_2088327894, train_975...</td>\n      <td>[train_2406599165, train_3576714541, train_270...</td>\n      <td>0.019231</td>\n      <td>0.181818</td>\n      <td>[train_2406599165, train_3576714541, train_150...</td>\n      <td>train_2406599165 train_3576714541 train_150810...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>00136d1cf4edede0203f32f05f660588.jpg</td>\n      <td>a6f319f924ad708c</td>\n      <td>Nescafe \\xc3\\x89clair Latte 220ml</td>\n      <td>3648931069</td>\n      <td>[train_3369186413, train_921438619]</td>\n      <td>[train_3369186413, train_2223127215, train_105...</td>\n      <td>[train_3369186413]</td>\n      <td>0.100000</td>\n      <td>0.666667</td>\n      <td>[train_3369186413]</td>\n      <td>train_3369186413</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"df[['posting_id', 'matches']].to_csv('submission.csv', index=False)\nsubmission_df = pd.read_csv('submission.csv')\nsubmission_df.head()","metadata":{"trusted":true},"execution_count":31,"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"         posting_id                                            matches\n0   train_129225211                   train_129225211 train_2278313361\n1  train_3386243561  train_3386243561 train_860027362 train_7788161...\n2  train_2288590299                  train_2288590299 train_3803689425\n3  train_2406599165  train_2406599165 train_3576714541 train_150810...\n4  train_3369186413                                   train_3369186413","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>posting_id</th>\n      <th>matches</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>train_129225211</td>\n      <td>train_129225211 train_2278313361</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>train_3386243561</td>\n      <td>train_3386243561 train_860027362 train_7788161...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>train_2288590299</td>\n      <td>train_2288590299 train_3803689425</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>train_2406599165</td>\n      <td>train_2406599165 train_3576714541 train_150810...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>train_3369186413</td>\n      <td>train_3369186413</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"markdown","source":"# Conclusion\n\nAnd we're done! We can now predict which postings are the same based on their titles. This approach would give a public score of 0.588.\n\nThe next steps would be to try to predict using the `image_phash` and the actual image. We can also combine the results of the 3 methods and see if accuracy will improve","metadata":{}}]}